{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12c30903-3285-4190-9931-6108d614e2b2",
   "metadata": {},
   "source": [
    "Q1. The mathematical formula for a linear Support Vector Machine (SVM) is represented as:\n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "f(x)=w \n",
    "T\n",
    " x+b\n",
    "\n",
    "where:\n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "f(x) is the decision function.\n",
    "�\n",
    "x represents the input features.\n",
    "�\n",
    "w is the weight vector.\n",
    "�\n",
    "b is the bias term.\n",
    "Q2. The objective function of a linear SVM aims to maximize the margin between the classes while minimizing the classification error. Mathematically, it can be formulated as:\n",
    "\n",
    "min\n",
    "⁡\n",
    "�\n",
    ",\n",
    "�\n",
    "1\n",
    "2\n",
    "∥\n",
    "�\n",
    "∥\n",
    "2\n",
    "min \n",
    "w,b\n",
    "​\n",
    "  \n",
    "2\n",
    "1\n",
    "​\n",
    " ∥w∥ \n",
    "2\n",
    " \n",
    "\n",
    "subject to the constraints:\n",
    "\n",
    "�\n",
    "�\n",
    "(\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    ")\n",
    "≥\n",
    "1\n",
    " for all \n",
    "�\n",
    "=\n",
    "1\n",
    ",\n",
    "2\n",
    ",\n",
    ".\n",
    ".\n",
    ".\n",
    ",\n",
    "�\n",
    "y \n",
    "i\n",
    "​\n",
    " (w \n",
    "T\n",
    " x \n",
    "i\n",
    "​\n",
    " +b)≥1 for all i=1,2,...,n\n",
    "\n",
    "where:\n",
    "\n",
    "�\n",
    "w is the weight vector.\n",
    "�\n",
    "b is the bias term.\n",
    "�\n",
    "�\n",
    "x \n",
    "i\n",
    "​\n",
    "  are the input feature vectors.\n",
    "�\n",
    "�\n",
    "y \n",
    "i\n",
    "​\n",
    "  are the class labels.\n",
    "�\n",
    "n is the number of training samples.\n",
    "Q3. The kernel trick in SVM allows the algorithm to implicitly map the input features into higher-dimensional feature spaces without actually calculating the transformation explicitly. This is achieved by replacing the dot product in the linear SVM formulation with a kernel function, such as polynomial kernel, Gaussian kernel (RBF), or sigmoid kernel. The kernel trick enables SVM to learn non-linear decision boundaries efficiently.\n",
    "\n",
    "Q4. Support vectors are the data points that lie closest to the decision boundary (hyperplane) in an SVM. They play a crucial role in defining the decision boundary and determining the margin. In practical terms, these are the instances that are most difficult to classify or lie on the margin. Support vectors directly influence the position and orientation of the decision boundary. In the case of a linear SVM, the decision boundary is defined by the support vectors and is equidistant from them.\n",
    "\n",
    "For example, consider a binary classification problem with two classes, represented by circles and crosses. In the following diagram, the support vectors are represented by the filled circles and crosses. The decision boundary (hyperplane) is defined by these support vectors, and it separates the two classes.\n",
    "\n",
    "[Diagram: Linear SVM with Support Vectors]\n",
    "\n",
    "Q5.\n",
    "\n",
    "Hyperplane: In SVM, the hyperplane is the decision boundary that separates the classes. For a binary classification problem, it is a (d-1)-dimensional subspace of the d-dimensional feature space. In a 2D feature space, the hyperplane is a line, and in a 3D feature space, it is a plane. In higher dimensions, it is a hyperplane. The hyperplane is defined by the equation \n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "=\n",
    "0\n",
    "w \n",
    "T\n",
    " x+b=0, where \n",
    "�\n",
    "w is the weight vector, \n",
    "�\n",
    "x is the feature vector, and \n",
    "�\n",
    "b is the bias term.\n",
    "\n",
    "Marginal Plane: The marginal plane, also known as the margin, is the region bounded by the support vectors on both sides of the hyperplane. It represents the maximum separation between the classes. In a linear SVM, the goal is to maximize the margin, which is the distance between the support vectors and the hyperplane.\n",
    "\n",
    "Soft Margin: In a soft-margin SVM, the margin is allowed to be violated by some samples to accommodate for noise or overlapping classes. It introduces a penalty parameter \n",
    "�\n",
    "C that controls the trade-off between maximizing the margin and minimizing the classification error. A higher value of \n",
    "�\n",
    "C leads to a narrower margin and potentially overfitting, while a lower value allows more margin violations.\n",
    "\n",
    "Hard Margin: In contrast, a hard-margin SVM aims to find the maximum-margin hyperplane without allowing any margin violations. It assumes that the data is perfectly separable. However, in real-world scenarios, data is often not linearly separable, and a hard-margin SVM may fail or lead to overfitting.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deab0082-a1b8-4a18-a097-a782c53c4fff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
